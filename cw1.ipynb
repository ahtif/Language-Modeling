{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS918 - CW1 Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumptions:\n",
    "* The corpus is expected to be named 'signal-news1.jsonl' and should be present in the same folder as the python script\n",
    "* The nltk modules are assumed to be in the path 'modules/cs918/nltk_data/'\n",
    "\n",
    "#### Notes:\n",
    "* The script is written in python 3 and there should be no issues running it with python 3, however since I did this assignment on the lab machines in dcs, I couldn't get jupyter working with a python 3 kernel, so I could only test if this works with python2.\n",
    "* If you are going to run this notebook with python3, then please remove or comment out the \\_\\_future\\_\\_ import below.\n",
    "* The running time of the script/notebook for me on the dcs lab machines is about 5 minutes with the POS tagger disabled and 10 minutes with the POS tagger enabled. If you want to run the script/notebook with the POS tagger enabled, then please uncomment the lines in the tag_lemmatize method\n",
    "* The perplexity of my model is about 357 without the POS tagger and 318 with the POS tagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports + Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import timeit\n",
    "from __future__ import print_function \n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.data.path.append('/modules/cs918/nltk_data/')\n",
    "UNK = \"<UNK>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is read all the JSON data into a list and then add each line of content separately into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"signal-news1.jsonl\") as f:\n",
    "    json_file = f.readlines()\n",
    "\n",
    "raw_content = []\n",
    "for line in json_file:\n",
    "    raw_content.append(json.loads(line)['content'].lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text\n",
    "We then preprocess the raw content, story by story using regex.\n",
    "First, all of the URLs are removed, then all of the numbers, the all non alphanumeric characters and finally, words made up of only 1 character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    urls = re.compile(r\"\\b(https?://)?([a-z_\\-0-9]+)(\\.[a-z_\\-0-9]+)+(/\\S*)*\\b\")\n",
    "    content = [re.sub(urls, \"\", x) for x in content]\n",
    "    #print (\"printing content without urls \\n\", content[417])\n",
    "\n",
    "    digits = re.compile(r\"\\b[0-9]+\\b\")\n",
    "    content = [re.sub(digits, \"\", x) for x in content]\n",
    "    #print (\"printing content without full numbers \\n\", content[417])\n",
    "\n",
    "    alpha = re.compile(r\"[^a-z0-9 ]\")\n",
    "    content = [re.sub(alpha, \"\", x) for x in content]\n",
    "    #print (\"printing only alphachars and spaces \\n\", content[417])\n",
    "\n",
    "    onechar = re.compile(r\"\\b\\w{1}\\b\")\n",
    "    content = [re.sub(onechar, \"\", x) for x in content]\n",
    "    #print (\"printing word with more than 1 chars \\n\", content[417])\n",
    "\n",
    "    return content    \n",
    "    \n",
    "content = preprocess(raw_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Lemmatizing the content\n",
    "Then all of the content is split up and tokenised into a list of lists. This is to keep content for each story seperate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_tokens = [] # keep lists of content seperate\n",
    "for l in content:\n",
    "    raw_tokens.append(l.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenised words are then lemmatized and then stored into a new list\n",
    "The words can also be POS-tagged before being lemmatized by just uncommenting out the lines in the tag_lemmatize method and commenting out the last line. Since the POS tags generate by the PerceptronTagger are treebank tags, they first need to be converted to the more simple wordnet tags before they can be used by the WordNetLemmatizer. This is done in the get_wordnet_tag function.\n",
    "However, I have disabled this since it adds about 5 extra minutes to the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wordnet_tag(treebank):\n",
    "    if treebank.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def tag_lemmatize(wnl, tagger, raw_tokens):\n",
    "    wordnet_tags = []\n",
    "    #for word in raw_tokens:\n",
    "    #    word, treebank = tagger.tag([word])[0]\n",
    "    #    wordnet_tags.append((word, get_wordnet_tag(treebank)))\n",
    "    #return [wnl.lemmatize(word, tag) for word, tag in wordnet_tags]\n",
    "    return [wnl.lemmatize(word) for word in raw_tokens]\n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "tagger = PerceptronTagger()\n",
    "tokens = []\n",
    "for l in raw_tokens:\n",
    "    tokenized_story = tag_lemmatize(wnl, tagger, l)\n",
    "    tokens.append(tokenized_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the vocabulary \n",
    "We then compute the vocabulary of the corpus, once again we keep it separate for each story.\n",
    "This is done using dictionaries, where each key is a unique token and its value is the number of times it appears in the  the specific story.\n",
    "From here it is trivial to compute the number of tokens and the vocabulary size. \n",
    "For the number of tokens, we sum the counts for each story to get a list of the number of tokens in each story. From there the number of tokens in the whole corpus is sum the list generated.\n",
    "Similary for the vocabulary size, we just get the list of keys for each story, and add them to a set. We can then find the length of that set in order to compute the size of the vocabulary size of the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for story in tokens:\n",
    "    story_vocab = Counter()\n",
    "    for t in story:\n",
    "        story_vocab[t] += 1\n",
    "    vocab.append(story_vocab)\n",
    "\n",
    "#Number of tokens N\n",
    "print (\"Number of tokens: \", sum([sum(v.values()) for v in vocab]))\n",
    "#Vocabulary size\n",
    "print (\"Size of vocab: \", len({word for v in vocab for word in v.keys()}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Trigrams\n",
    "To compute the top 25 trigrams, we first need to compute all of the trigrams and their respective counts. This is done in a similar way to how we generate the vocabulary.\n",
    "For each tokenized story, we generate all the trigrams for that story using the nltk.trigram function. We then add each of these trigrams to a dictionary which and update their count's if they reoccur. This is done for each story in the corpus so we end up with a dictionary that contains the counts for each trigram present in the corpus.\n",
    "From here we can easily find the top 25 trigrams by simply sorting the dictionary by its values, and then taking the top 25 from that list.\n",
    "\n",
    "#### Note: We have also generated the trigram and bigram counts per story aswell, these will be used when generating the language models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Top 25 trigrams\n",
    "all_trigrams = Counter()\n",
    "story_trigrams = []\n",
    "story_bigrams = []\n",
    "for story in tokens:\n",
    "    tlist = nltk.trigrams(story)\n",
    "    trigrams = Counter()\n",
    "    for trigram in tlist:\n",
    "        trigrams[trigram] += 1\n",
    "    all_trigrams.update(trigrams)\n",
    "    story_trigrams.append(trigrams)\n",
    "\n",
    "    blist = nltk.bigrams(story)\n",
    "    bigrams = Counter()\n",
    "    for bigram in blist:\n",
    "        bigrams[bigram] += 1\n",
    "    story_bigrams.append(bigrams)\n",
    "\n",
    "print(\"Top 25 trigrams :\", sorted(list(all_trigrams.items()), key=lambda x: x[1], reverse=True)[:25])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing positive and negative counts\n",
    "Before we can compute the positive and negative counts in the corpus, we must first read in the positive and negative words into seperate lists and preprocess them similary to how the content is proprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Positive and negative counts\n",
    "with open(\"positive-words.txt\", \"r\") as pos:\n",
    "    positives = set()\n",
    "    for word in pos:\n",
    "        positives.add(re.sub(r\"[^a-z0-9 ]\", \"\", word.rstrip(\"\\n\")))\n",
    "\n",
    "with open(\"negative-words.txt\", \"r\") as neg:\n",
    "    negatives = set()\n",
    "    for word in neg:\n",
    "        negatives.add(re.sub(r\"[^a-z0-9 ]\", \"\", word.rstrip(\"\\n\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have already computed the vocabulary for each story, we can simply iterate through word in a particular story, check whether it is positive or negative (or neither) and then add it's count the number of positive or negative words per story.\n",
    "Once we have the positive and negative counts per story, we can then decide whether a story is positive or negative and increase the count of either negative or positive stories and then finally add the local positive and negative counts to the global ones so we can compute the total negative and positive counts of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_stories = 0\n",
    "neg_stories = 0\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "\n",
    "def count_pos_neg(positives, negatives, vocab): \n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for token, count in vocab.items():\n",
    "        if token in positives:\n",
    "            pos_count += count\n",
    "        if token in negatives:\n",
    "            neg_count += count\n",
    "    return pos_count, neg_count\n",
    "\n",
    "for story in vocab:\n",
    "    local_pos, local_neg = count_pos_neg(positives, negatives, story)\n",
    "    if local_pos > local_neg:\n",
    "        pos_stories += 1\n",
    "    if local_pos < local_neg:\n",
    "        neg_stories += 1\n",
    "    pos_count += local_pos\n",
    "    neg_count += local_neg\n",
    "    \n",
    "print(\"Number of positive words: {}, Number of negative words: {}\".format(pos_count, neg_count))\n",
    "print(\"Number of positive stories: {}, Number of negative stories: {}\".format(pos_stories, neg_stories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Language Models\n",
    "### Creating the models\n",
    "Since we are using \"stupid backoff\" for our trigram language model we create 3 models, a trigram model, a bigram model and a unigram model from the 16000 items in the corpus. These are constructed using the per story trigram and bigram counts that we computed earlier\n",
    "Also in the unigram model, we replace all tokens with a count of 1 or less to the \"UNK\" token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Part C: Language Models\n",
    "tri_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "bi_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "uni_model = defaultdict(lambda: 0)\n",
    "\n",
    "for i in range(0, 16000):\n",
    "    for (x, y, z), count in story_trigrams[i].items():\n",
    "        tri_model[(x, y)][z] += count\n",
    "    for (x, y), count in story_bigrams[i].items():\n",
    "        bi_model[x][y] += count\n",
    "    for t, count in vocab[i].items():\n",
    "        if count <= 1:\n",
    "            uni_model[UNK] += count\n",
    "        else:\n",
    "            uni_model[t] += count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming a sentence\n",
    "Once we have computed our language models, we can form a sentence with the bigram (\"is\",\"this\") by using our trigram model to get all the possible trigrams in our model that start with the bigram, then sorting the list produced to find the most likely one. We then append the most likely word to our sentence and then change our bigram to last two words in the sentence and continue the same process untill we get a sentence with 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def form_sentence(tri_model, bigram, length):\n",
    "    i = 0\n",
    "    sentence = [bigram[0], bigram[1]]\n",
    "    while len(sentence) < length:\n",
    "        all_trigrams = tri_model[(sentence[i], sentence[i+1])]\n",
    "        most_likely = sorted(all_trigrams.items(), key=lambda x: x[1], reverse=True)[0]\n",
    "        sentence.append(most_likely[0])\n",
    "        i += 1\n",
    "    return sentence\n",
    "\n",
    "bigram = (\"is\",\"this\")\n",
    "bigram = tag_lemmatize(wnl, tagger, bigram)\n",
    "print (*form_sentence(tri_model, bigram, 10), sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Perplexity\n",
    "To compute perplexity we first need to be able to compute the probability of a trigram in our model. We do this using \"stupid\" backoff, which is as follows:\n",
    "If the trigram (w1, w2, w3) exists in the in model, then return the MLE(Maximum Likelihood Estimate) for that trigram.\n",
    "If not, we check if the bigram (w2, w3) exists, then return the MLE of that trigram, but but multiplied by the alpha value of 0.4\n",
    "If the bigram does not exist either, we then, return MLE for the unigram (w3) instead, but this time the alpha is 0.4 \\* 0.4.\n",
    "If still, the unigram does not exist in the model either, we then return the MLE of the UNK token, again multiplier by the alpha of 0.4 \\* 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stupid_backoff(tri_model, bi_model, uni_model, trigram):\n",
    "    stupid = 0.4\n",
    "    x, y, z = trigram\n",
    "    if tri_model[(x, y)][z] > 0:\n",
    "        return tri_model[(x, y)][z]/bi_model[x][y]\n",
    "    elif bi_model[y][z] > 0:\n",
    "        return stupid * (bi_model[y][z]/sum(bi_model[y].values()))\n",
    "    elif uni_model[z] > 0:\n",
    "        return stupid*stupid*(uni_model[z]/sum(uni_model.values()))\n",
    "    else:\n",
    "        return stupid*stupid*(uni_model[UNK]/sum(uni_model.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we can compute the probability of a trigram, we can now evaluate the perplexity of the model against the remaing rows of the corpus.\n",
    "The perplexity is calculated as 2 to power of minus 1 over the sum of the log probabilities for each trigram in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_perplexity(tri_model, bi_model, uni_model, test):\n",
    "    perplexity = 0.0\n",
    "    num_trigrams = 0\n",
    "    unseen = 0\n",
    "    test_trigrams = Counter()\n",
    "    for row in test:\n",
    "        test_trigrams.update(row)\n",
    "\n",
    "    for trigram, count in test_trigrams.items():\n",
    "        prob = stupid_backoff(tri_model, bi_model, uni_model, trigram)\n",
    "        if prob != 0:\n",
    "            perplexity += count*math.log(prob, 2)\n",
    "        num_trigrams += count\n",
    "    return math.pow(2, -(perplexity/num_trigrams))\n",
    "\n",
    "print(\"perplexity of model is: \", calc_perplexity(tri_model, bi_model, uni_model, story_trigrams[16000:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
